{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_size, random_state=0):\n",
    "    # setting a random state \n",
    "    random.seed(random_state)\n",
    "\n",
    "    # check if the testsize if fload or not\n",
    "    if isinstance(train_size, float):\n",
    "        if (1.0 > train_size > 0.0):\n",
    "            # rounding to get the right portion for the split\n",
    "            train_size = round(train_size * len(df))\n",
    "        else:\n",
    "            print(\"The Test Size that u have choosed is not applicable choose from the range of [0,1]\")\n",
    "\n",
    "    # df.index  returns RangeIndex(start=0, stop=150, step=1) // returns the indices of df\n",
    "    # so we need to change that to a list to we can pick our indices\n",
    "    indices = df.index.tolist()\n",
    "    # picking our random sample\n",
    "    train_indices = random.sample(population=indices, k=train_size)\n",
    "\n",
    "    # df.loc assign allocated samples with specific indices to a df \n",
    "    train_df = df.loc[train_indices]\n",
    "    # df.drop removes those samples from the df\n",
    "    test_df = df.drop(train_indices)\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Purity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if the data has only one classification or not\n",
    "def check_purity(data):\n",
    "\n",
    "    # access only the last column of the dataset\n",
    "    label_column = data[:, -1]\n",
    "\n",
    "    # all the uniuqe classifications are saved into one array \n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    # checking if the classification array has only one class\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "\n",
    "    label_column = data[:, -1]\n",
    "\n",
    "    # the unique classes are saved into array and they counts into counts array\n",
    "    unique_classes, counts_unique_classes = np.unique(\n",
    "        label_column, return_counts=True)\n",
    "\n",
    "    # to get the index of the class that has more samples in it\n",
    "    index = counts_unique_classes.argmax()\n",
    "    # returning the major class\n",
    "    classification = unique_classes[index]\n",
    "\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Potential Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "\n",
    "    potenial_splits = {}  # creating an empty list\n",
    "    _, n_columns = data.shape  # getting number of columns\n",
    "    for column_index in range(n_columns - 1):  # loop over these columns\n",
    "\n",
    "        # lets see those values and loop over them later\n",
    "        values = data[:, column_index]\n",
    "        # take only the unique values\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        feature_types = FEATURE_TYPES[column_index]\n",
    "\n",
    "        if feature_types == \"continious\":\n",
    "            # creats a set with the number of columns in the data likest this : {0: [], 1: [], 2: [], 3: []}\n",
    "            potenial_splits[column_index] = []\n",
    "            # looping on the on the indicies of these unique values\n",
    "            for index in range(len(unique_values)):\n",
    "                # breaking loop condition for (unique_values[0])\n",
    "                if index != 0:\n",
    "                    # creating a split between those unique values\n",
    "                    current_value = unique_values[index]\n",
    "                    previous_value = unique_values[index - 1]\n",
    "                    # if unique value are 0.2 & 0.3 the split will be at (0.3+0.2)/2 : 2.5\n",
    "                    split = ((current_value + previous_value) / 2)\n",
    "                    # back to the list we created : {0: [], 1: [], 2: [], 3: []}\n",
    "                    # all splits will be appended here  ^      ^      ^      ^\n",
    "                    # after each full loop potenial_splits should have one group filled with splits and other intiated {0: [1, 2, 3], 1: [] }\n",
    "                    potenial_splits[column_index].append(split)\n",
    "\n",
    "        else:\n",
    "            potenial_splits[column_index] = unique_values\n",
    "\n",
    "    return potenial_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitter using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "\n",
    "    # split_column is a colunm in our data [ feature values]\n",
    "    # split_value is a value choosen to split that column\n",
    "\n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    feature_types = FEATURE_TYPES[split_column]\n",
    "\n",
    "    if feature_types == \"continious\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values > split_value]\n",
    "\n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "\n",
    "    return data_below, data_above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulated_entropy(data):\n",
    "\n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()   # p(A) = #of A / total samples \n",
    "    entorpy = sum(probabilities * -np.log2(probabilities)) # entropy low\n",
    "\n",
    "    return entorpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulated_Overall_entropy(data_below, data_above):\n",
    "\n",
    "    n_data_points = len(data_below) + len(data_above)\n",
    "\n",
    "    p_data_below = len(data_below) / n_data_points\n",
    "    p_data_above = len(data_above) / n_data_points\n",
    "\n",
    "    overall_entropy = (p_data_below * caclulated_entropy(data_below)\n",
    "                       + p_data_above * caclulated_entropy(data_above))\n",
    "\n",
    "    return overall_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits):\n",
    "\n",
    "    # a way to determine the best split is by measuring the intropy of the that split\n",
    "    overall_entropy = 999\n",
    "    # loop over the potential splits {set}\n",
    "    for column_index in potential_splits:\n",
    "        # loop over the values of the splits in each column inside the set\n",
    "        for value in potential_splits[column_index]:\n",
    "            # split data and measure the overall entropy\n",
    "            data_below, data_above = split_data(\n",
    "                data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = caclulated_Overall_entropy(\n",
    "                data_below, data_above)\n",
    "            # choose the split with the lowest overall entropy\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "\n",
    "    return best_split_column, best_split_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Feature Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Determine_feature_type(df):\n",
    "    \n",
    "    feature_types = []\n",
    "    n_unique_values_threshold = 15\n",
    "\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        example_value = unique_values[0]\n",
    "\n",
    "        if(isinstance(example_value, str) or len(unique_values)<= n_unique_values_threshold):\n",
    "            feature_types.append(\"categorical\")\n",
    "\n",
    "        else:\n",
    "            feature_types.append(\"continious\")\n",
    "\n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decsion Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorthim(df, counter=0, min_samples=2, max_depth=5):\n",
    "\n",
    "\n",
    "    \n",
    "    # data_preparation\n",
    "    if counter == 0:\n",
    "        # save the feature names and feature type [ categorical or continious]\n",
    "\n",
    "        # since at default we want work with np arrays so values are changed\n",
    "        data = df.values\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        global FEATURE_TYPES\n",
    "        FEATURE_TYPES = Determine_feature_type(df)\n",
    "\n",
    "    else:\n",
    "        # as the counter goes up for the recursion part data is saves as df to enter the recursion\n",
    "        data = df\n",
    "\n",
    "    # returning the classification ither when there only\n",
    "        # 1- data is pure\n",
    "        # 2 - pruning\n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        return classification\n",
    "\n",
    "    # recursive part\n",
    "    else:\n",
    "        # when data is not pure or no pruning is happening counter is increased so data can be splited again\n",
    "        counter += 1\n",
    "\n",
    "        # re splitting of the data to classify it\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(\n",
    "            data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "\n",
    "        # creation of tree\n",
    "        # choosing the best feature for the decsion - again the split_columns are the features\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        feature_types = FEATURE_TYPES[split_column]\n",
    "\n",
    "        if feature_types == \"continious\":\n",
    "            # creating the question that will indicate the classification\n",
    "            question = \"{} <= {} \".format(feature_name, split_value)\n",
    "\n",
    "        else:\n",
    "            question = \"{} == {} \".format(feature_name, split_value)\n",
    "        # the sub_tree is a set , branch , so over all either imagine it as\n",
    "        # 1- tree with branches\n",
    "        # 2- set with sub_sets\n",
    "        sub_tree = {question: []}\n",
    "\n",
    "        # creating the left and right path for the tree - and recurse on them\n",
    "        # true on the left ,,,, False on the right\n",
    "        yes_answer = decision_tree_algorthim(\n",
    "            data_below, counter, min_samples, max_depth)\n",
    "        no_answer = decision_tree_algorthim(\n",
    "            data_above, counter, min_samples, max_depth)\n",
    "\n",
    "        # incase of the rght and left answers are both the same thing we replace the sub_tree with just a leaf\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            # appedning the subtree to the main tree\n",
    "            sub_tree[question].append(no_answer)\n",
    "            sub_tree[question].append(yes_answer)\n",
    "\n",
    "\n",
    "        return sub_tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(example, tree):\n",
    "\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comarsio_opperator, value = question.split()\n",
    "\n",
    "    # check if the feature contained in that classification or not\n",
    "\n",
    "    if comarsio_opperator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            # if yes go left [answer = yes_answer]\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            # if no go right [answer = no_answer]\n",
    "            answer= tree[question][1]\n",
    "\n",
    "    else: \n",
    "        if str(example[feature_name]) == value:\n",
    "            # if yes go left [answer = yes_answer]\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            # if no go right [answer = no_answer]\n",
    "            answer= tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    # check if the answer is a dictionary value or not \n",
    "    # if it's a dict value classification is returned \n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "\n",
    "    # if not recurse till you get only a dictionary value\n",
    "    # recursion part\n",
    "    else:\n",
    "    # cut tree and recurse in the sub_tree\n",
    "        residual_tree = answer\n",
    "        return classify_sample(example, residual_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy (df, tree):\n",
    "\n",
    "    # accessing the last column in the dataframe and save it\n",
    "    last_column = df[df.columns[-1]]\n",
    "\n",
    "    # creating a new data frame for classification with the classes only\n",
    "    classification_df = pd.DataFrame(last_column)\n",
    "\n",
    "    # adding a column of predictions to the dataframe\n",
    "    classification_df[\"classification\"] = df.apply(\n",
    "        classify_sample, axis=1, args=(tree,), )\n",
    "\n",
    "    # check the predictions\n",
    "    classification_df[\"classification_correct\"] = classification_df.iloc[:,0] == classification_df.iloc[:, 1]\n",
    "    # calculating the accuracy\n",
    "    accuracy = classification_df.classification_correct.mean()\n",
    "    \n",
    "        \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree):\n",
    "\n",
    "    pprint(tree)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbc768028c3e6ead51d9a200ddcb2ec858ae62844dcd1994729a8279be9b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
